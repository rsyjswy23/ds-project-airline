{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch training notebook:\n",
    "\n",
    "- Loads a small sample of the transformed dataset (from `default.flights_2006_transformed` or parquet path),\n",
    "- Converts to pandas and then to torch tensors,\n",
    "- Trains a tiny feedforward network to predict `ArrDelay` (regression),\n",
    "- Reports train/test RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.13.1+cpu)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.14.1+cpu)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.18.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.24.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (7.6.0)\n",
      "Requirement already satisfied: nbclient in /opt/conda/lib/python3.7/site-packages (0.7.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from nbconvert) (4.9.1)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert) (3.1.5)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert) (0.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/conda/lib/python3.7/site-packages (from nbconvert) (6.7.0)\n",
      "Requirement already satisfied: jinja2>=3.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert) (3.1.6)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /opt/conda/lib/python3.7/site-packages (from nbconvert) (4.12.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert) (0.2.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /opt/conda/lib/python3.7/site-packages (from nbconvert) (5.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from nbconvert) (20.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert) (1.4.2)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert) (2.6.1)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.7/site-packages (from nbconvert) (1.2.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert) (5.9.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.7/site-packages (from nbclient) (7.4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from bleach!=5.0.0->nbconvert) (1.15.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=3.6->nbconvert) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=3.6->nbconvert) (4.7.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->nbclient) (0.3)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.4 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->nbclient) (1.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->nbclient) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->nbclient) (26.2.1)\n",
      "Requirement already satisfied: tornado>=6.2 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->nbclient) (6.2)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.7/site-packages (from nbformat>=5.7->nbconvert) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.7/site-packages (from nbformat>=5.7->nbconvert) (3.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->nbconvert) (2.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->nbconvert) (2.4.7)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (68.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --no-cache-dir nbconvert nbclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "print(torch.version.cuda)          # Should show a CUDA version\n",
    "print(torch.cuda.is_available())   # Should return True\n",
    "print(torch.cuda.device_count())   # Number of GPUs visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from local path: models/pytorch_small_regressor.pth\n",
      "Loaded checkpoint via: local\n",
      "\n",
      "Checkpoint top-level keys: ['weight', 'bias']\n",
      "Found weight key: weight shape: (1, 6)\n",
      "Inferred input features = 6\n",
      "State dict loaded into nn.Linear\n",
      "Smoke forward ok, output shape = (2, 1) sample outputs = [0.8670730590820312, 0.35125476121902466]\n",
      "\n",
      "If you want to inspect scaler in the checkpoint (if present):\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os, io, traceback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "local_path = 'models/pytorch_small_regressor.pth'\n",
    "\n",
    "def try_load_local(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    print('Loading from local path:', path)\n",
    "    data = torch.load(path, map_location='cpu')\n",
    "    return data\n",
    "\n",
    "def try_load_via_spark_copy(local_target=local_path, hdfs_src='/data/models/pytorch_small_regressor.pth'):\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        try:\n",
    "            spark\n",
    "        except NameError:\n",
    "            spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "        sc = spark.sparkContext\n",
    "        jconf = sc._jsc.hadoopConfiguration()\n",
    "        fs_default = jconf.get('fs.defaultFS')\n",
    "        if not fs_default or fs_default.startswith('file://'):\n",
    "            jconf.set('fs.defaultFS', 'hdfs://namenode:8020')\n",
    "        fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(jconf)\n",
    "        Path = sc._jvm.org.apache.hadoop.fs.Path\n",
    "        src = Path(hdfs_src)\n",
    "        dst = Path('file://' + os.path.abspath(local_target))\n",
    "        os.makedirs(os.path.dirname(local_target), exist_ok=True)\n",
    "        print('Copying from HDFS to local:', hdfs_src, '->', local_target)\n",
    "        fs.copyToLocalFile(False, src, dst)\n",
    "        return try_load_local(local_target)\n",
    "    except Exception:\n",
    "        print('Spark-based HDFS copy failed:')\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def try_load_via_pyarrow(hdfs_src='/data/models/pytorch_small_regressor.pth'):\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        print('Attempting to read from HDFS via pyarrow:', hdfs_src)\n",
    "        fs = pa.hdfs.connect('namenode', 8020)\n",
    "        with fs.open(hdfs_src, 'rb') as f:\n",
    "            data_bytes = f.read()\n",
    "        return torch.load(io.BytesIO(data_bytes), map_location='cpu')\n",
    "    except Exception:\n",
    "        print('pyarrow HDFS read failed:')\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "loader_sequence = [\n",
    "    ('local', try_load_local),\n",
    "    ('spark_copy', try_load_via_spark_copy),\n",
    "    ('pyarrow', try_load_via_pyarrow),\n",
    "]\n",
    "\n",
    "loaded = None\n",
    "for name, fn in loader_sequence:\n",
    "    try:\n",
    "        loaded = fn(local_path)\n",
    "        print('Loaded checkpoint via:', name)\n",
    "        break\n",
    "    except FileNotFoundError:\n",
    "        print('Not found locally for loader:', name)\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print('Loader', name, 'failed with:', type(e).__name__, e)\n",
    "        continue\n",
    "\n",
    "if loaded is None:\n",
    "    raise RuntimeError('All loading strategies failed; cannot load model checkpoint.')\n",
    "\n",
    "print('\\nCheckpoint top-level keys:', list(loaded.keys()) if isinstance(loaded, dict) else 'non-dict checkpoint')\n",
    "\n",
    "# Extract model_state_dict if present\n",
    "state = loaded.get('model_state_dict') if isinstance(loaded, dict) and 'model_state_dict' in loaded else loaded\n",
    "if not isinstance(state, dict):\n",
    "    raise RuntimeError('Checkpoint does not contain a state dict.')\n",
    "\n",
    "# Find a weight tensor to infer input dim\n",
    "weight_tensor = None\n",
    "for k, v in state.items():\n",
    "    if 'weight' in k and hasattr(v, 'shape'):\n",
    "        weight_tensor = v\n",
    "        print('Found weight key:', k, 'shape:', tuple(v.shape))\n",
    "        break\n",
    "\n",
    "if weight_tensor is None:\n",
    "    raise RuntimeError('Could not find weight tensor in state dict to infer model shape')\n",
    "\n",
    "in_features = weight_tensor.shape[1]\n",
    "print('Inferred input features =', in_features)\n",
    "\n",
    "# Reconstruct a single Linear layer and load state (for simple models)\n",
    "model = nn.Linear(in_features, 1)\n",
    "model_state = {k.replace('module.', ''): v for k, v in state.items()}  # handle possible DataParallel prefix\n",
    "try:\n",
    "    model.load_state_dict(model_state)\n",
    "    print('State dict loaded into nn.Linear')\n",
    "except Exception:\n",
    "    print('Full load failed; attempting to load matching keys only')\n",
    "    ms = model.state_dict()\n",
    "    matched = {k: v for k, v in model_state.items() if k in ms and ms[k].shape == v.shape}\n",
    "    ms.update(matched)\n",
    "    model.load_state_dict(ms)\n",
    "    print('Partial state loaded (matched keys).')\n",
    "\n",
    "# Smoke test inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    import numpy as np\n",
    "    x = torch.from_numpy(np.random.randn(2, in_features).astype('float32'))\n",
    "    y = model(x)\n",
    "    print('Smoke forward ok, output shape =', tuple(y.shape), 'sample outputs =', y.reshape(-1).tolist())\n",
    "\n",
    "print('\\nIf you want to inspect scaler in the checkpoint (if present):')\n",
    "if isinstance(loaded, dict) and 'scaler' in loaded:\n",
    "    print('scaler present in checkpoint (object type):', type(loaded['scaler']))\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded table from metastore: default.flights_2006_transformed\n",
      "Sample rows (from metastore sampling): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: Metastore sampling returned 0 rows; will try parquet fallback with larger fraction/limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parquet fallback: hdfs://namenode:8020/data/parquet/flights_2006_transformed\n",
      "Sample rows (from parquet sampling, fraction= 0.01 ): 70163\n",
      "Sample rows (pandas): 3000\n",
      "Features shape: (3000, 8)\n",
      "Prepared numpy arrays:\n",
      "  X_train: (2400, 8)\n",
      "  y_train: (2400,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# Initialize or reuse existing Spark session\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "\n",
    "# Configuration\n",
    "tbl_name = 'default.flights_2006_transformed'\n",
    "parquet_path = 'hdfs://namenode:8020/data/parquet/flights_2006_transformed'\n",
    "\n",
    "def load_small_sample(spark, tbl_name, parquet_path, sample_fraction=0.0005, max_rows=3000, seed=42):\n",
    "    \"\"\"Load a small sample from Hive metastore with parquet fallback.\"\"\"\n",
    "    df_spark = None\n",
    "\n",
    "    # Try loading from metastore\n",
    "    try:\n",
    "        df_spark = spark.table(tbl_name)\n",
    "        print('Loaded table from metastore:', tbl_name)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f'Metastore load failed: {e}; will try parquet fallback')\n",
    "\n",
    "    # Attempt sampling from metastore\n",
    "    if df_spark is not None:\n",
    "        df_sample = df_spark.sample(withReplacement=False, fraction=sample_fraction, seed=seed)\n",
    "        cnt = df_sample.count()\n",
    "        print('Sample rows (from metastore sampling):', cnt)\n",
    "        if cnt > 0:\n",
    "            return df_sample\n",
    "        else:\n",
    "            warnings.warn('Metastore sampling returned 0 rows; will try parquet fallback with larger fraction/limit')\n",
    "\n",
    "    # Parquet fallback\n",
    "    try:\n",
    "        df_par = spark.read.parquet(parquet_path)\n",
    "        print('Loaded parquet fallback:', parquet_path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'Failed to read parquet fallback {parquet_path}: {e}')\n",
    "\n",
    "    # Larger sample from parquet\n",
    "    larger_fraction = max(sample_fraction * 20, 0.001)\n",
    "    try:\n",
    "        df_sample = df_par.sample(withReplacement=False, fraction=larger_fraction, seed=seed)\n",
    "        cnt = df_sample.count()\n",
    "        print('Sample rows (from parquet sampling, fraction=', larger_fraction, '):', cnt)\n",
    "        if cnt > 0:\n",
    "            return df_sample.limit(max_rows)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f'Parquet sampling failed: {e}; will try direct limit()')\n",
    "\n",
    "    # Final fallback: direct limit()\n",
    "    try:\n",
    "        df_direct = df_par.limit(max_rows)\n",
    "        cnt = df_direct.count()\n",
    "        print('Sample rows (direct limit from parquet):', cnt)\n",
    "        if cnt > 0:\n",
    "            return df_direct\n",
    "    except Exception as e:\n",
    "        warnings.warn(f'Direct parquet limit() failed: {e}')\n",
    "\n",
    "    # Nothing found\n",
    "    return None\n",
    "\n",
    "\n",
    "# === Main script ===\n",
    "if __name__ == \"__main__\":\n",
    "    df_sample = load_small_sample(spark, tbl_name, parquet_path,\n",
    "                                  sample_fraction=0.0005, max_rows=3000, seed=42)\n",
    "\n",
    "    if df_sample is None:\n",
    "        print(\"\\nERROR: No rows returned from metastore or parquet fallback.\\n\")\n",
    "        print(\"Actions to investigate:\")\n",
    "        print(\" - Verify the Hive metastore table exists: spark.sql('SHOW TABLES IN default').show()\")\n",
    "        print(\" - Inspect the parquet path: hadoop fs -ls hdfs://namenode:8020/data/parquet/flights_2006_transformed\")\n",
    "        print(\" - Ensure fs.defaultFS is correctly configured for your SparkSession\")\n",
    "        raise RuntimeError(\"Could not obtain any sample rows from metastore or parquet fallback\")\n",
    "\n",
    "    # Convert to pandas safely (sample should be small)\n",
    "    df_pd = df_sample.toPandas()\n",
    "    print('Sample rows (pandas):', len(df_pd))\n",
    "\n",
    "    if len(df_pd) == 0:\n",
    "        raise RuntimeError('Pandas sample is empty after all fallbacks — aborting to avoid scaler errors')\n",
    "\n",
    "    # Ensure target exists and numeric extraction remains robust\n",
    "    if 'ArrDelay' not in df_pd.columns:\n",
    "        try:\n",
    "            df_pd['ArrDelay'] = pd.to_numeric(df_pd.get('ArrDelay', pd.Series(dtype=float)), errors='coerce')\n",
    "        except Exception:\n",
    "            df_pd['ArrDelay'] = pd.Series([None] * len(df_pd))\n",
    "\n",
    "    numeric = df_pd.select_dtypes(include=[np.number]).copy()\n",
    "    if 'ArrDelay' not in numeric.columns:\n",
    "        numeric['ArrDelay'] = pd.to_numeric(df_pd['ArrDelay'], errors='coerce')\n",
    "\n",
    "    # Prepare feature and target arrays\n",
    "    top_k = min(8, max(1, numeric.shape[1] - 1))\n",
    "    feat_cols = [c for c in numeric.columns if c != 'ArrDelay'][:top_k]\n",
    "    X = numeric[feat_cols].copy()\n",
    "    y = numeric['ArrDelay'].values.astype(np.float32) if 'ArrDelay' in numeric.columns else np.zeros(len(X), dtype=np.float32)\n",
    "\n",
    "    if len(X) == 0 or X.shape[1] == 0:\n",
    "        raise RuntimeError('No numeric features available after fallbacks. Check the transformed dataset contents.')\n",
    "\n",
    "    if len(X) > 3000:\n",
    "        sampled_idx = np.random.RandomState(42).choice(len(X), size=3000, replace=False)\n",
    "        X = X.iloc[sampled_idx].reset_index(drop=True)\n",
    "        y = y[sampled_idx]\n",
    "\n",
    "    print('Features shape:', X.shape)\n",
    "\n",
    "    # Split, scale, and prepare numpy arrays\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scaled = X_train_scaled.astype(np.float32)\n",
    "    X_test_scaled = X_test_scaled.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "\n",
    "    print('Prepared numpy arrays:')\n",
    "    print('  X_train:', X_train_scaled.shape)\n",
    "    print('  y_train:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from models/pytorch_small_regressor.pth\n",
      "Inferred model input features = 6\n",
      "Loaded full state dict into model\n",
      "Metastore sampling returned 0 rows; trying parquet fallback\n",
      "Loaded sample from parquet (sampled)\n",
      "Pandas sample rows: 3000\n",
      "No scaler in checkpoint; fitting StandardScaler on training split\n",
      "\n",
      "PyTorch baseline metrics:\n",
      " RMSE = 31.4780\n",
      " R2   = -0.0723\n",
      "\n",
      "Previous PySpark baselines:\n",
      " LinearRegression RMSE=11.2483, R2=0.9056\n",
      " RandomForest     RMSE=17.0213, R2=0.7837\n",
      "\n",
      "Comparison: lower RMSE and higher R² indicate better performance.\n",
      "\n",
      "Sample predictions:\n",
      " ArrDelay  prediction\n",
      "     -4.0    0.060429\n",
      "    -22.0    0.743498\n",
      "      2.0    0.907675\n",
      "     47.0    0.745521\n",
      "     -3.0    0.381357\n",
      "     73.0    0.833490\n",
      "    -15.0    0.293262\n",
      "     -4.0   -0.274133\n",
      "     87.0   -0.096472\n",
      "    -26.0    0.011505\n",
      "\n",
      "Done — use these metrics to compare with your earlier models.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------------------------\n",
    "local_ckpt = 'models/pytorch_small_regressor.pth'\n",
    "tbl_name = 'default.flights_2006_transformed'\n",
    "parquet_path = 'hdfs://namenode:8020/data/parquet/flights_2006_transformed'\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# -------------------------------------------------------------------\n",
    "def load_checkpoint(path):\n",
    "    \"\"\"Load a PyTorch checkpoint file.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    print('Loading checkpoint from', path)\n",
    "    return torch.load(path, map_location='cpu')\n",
    "\n",
    "\n",
    "def get_small_sample(spark, tbl_name, parquet_path, sample_fraction=0.0005, max_rows=3000, seed=42):\n",
    "    \"\"\"Try to get a small sample from Hive metastore; fallback to Parquet.\"\"\"\n",
    "    # Try metastore\n",
    "    try:\n",
    "        df_spark = spark.table(tbl_name)\n",
    "        df_sample = df_spark.sample(withReplacement=False, fraction=sample_fraction, seed=seed)\n",
    "        if df_sample.count() > 0:\n",
    "            print('Loaded sample from metastore (sampled)')\n",
    "            return df_sample\n",
    "        else:\n",
    "            print('Metastore sampling returned 0 rows; trying parquet fallback')\n",
    "    except Exception as e:\n",
    "        print('Metastore read failed, will try parquet fallback:', e)\n",
    "\n",
    "    # Parquet fallback\n",
    "    df_par = spark.read.parquet(parquet_path)\n",
    "    try:\n",
    "        larger_fraction = max(sample_fraction * 20, 0.001)\n",
    "        df_sample = df_par.sample(withReplacement=False, fraction=larger_fraction, seed=seed)\n",
    "        if df_sample.count() > 0:\n",
    "            print('Loaded sample from parquet (sampled)')\n",
    "            return df_sample.limit(max_rows)\n",
    "    except Exception as e:\n",
    "        print('Parquet sampling failed or empty, will try direct limit:', e)\n",
    "\n",
    "    # Last resort: direct limit()\n",
    "    df_direct = df_par.limit(max_rows)\n",
    "    if df_direct.count() > 0:\n",
    "        print('Loaded sample via direct parquet.limit()')\n",
    "        return df_direct\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Initialize Spark\n",
    "# -------------------------------------------------------------------\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Load checkpoint\n",
    "# -------------------------------------------------------------------\n",
    "try:\n",
    "    ckpt = load_checkpoint(local_ckpt)\n",
    "except FileNotFoundError:\n",
    "    raise RuntimeError(f'Local checkpoint not found at {local_ckpt}. '\n",
    "                       'Copy it from HDFS first or run the training step.')\n",
    "\n",
    "# Extract state dict\n",
    "state = ckpt.get('model_state_dict') if isinstance(ckpt, dict) and 'model_state_dict' in ckpt else ckpt\n",
    "if not isinstance(state, dict):\n",
    "    raise RuntimeError('Checkpoint did not contain a state dict')\n",
    "\n",
    "# Infer input dimension\n",
    "weight_tensor = None\n",
    "for k, v in state.items():\n",
    "    if 'weight' in k and hasattr(v, 'shape'):\n",
    "        weight_tensor = v\n",
    "        break\n",
    "if weight_tensor is None:\n",
    "    raise RuntimeError('Could not find weight tensor in checkpoint state dict')\n",
    "in_features = weight_tensor.shape[1]\n",
    "print('Inferred model input features =', in_features)\n",
    "\n",
    "# Build model\n",
    "model = nn.Linear(in_features, 1)\n",
    "try:\n",
    "    model.load_state_dict(state)\n",
    "    print('Loaded full state dict into model')\n",
    "except Exception:\n",
    "    ms = model.state_dict()\n",
    "    matched = {k: v for k, v in state.items() if k in ms and ms[k].shape == v.shape}\n",
    "    ms.update(matched)\n",
    "    model.load_state_dict(ms)\n",
    "    print('Loaded partial/matched state into model')\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Load sample data\n",
    "# -------------------------------------------------------------------\n",
    "df_sample = get_small_sample(spark, tbl_name, parquet_path, sample_fraction=0.0005, max_rows=3000, seed=42)\n",
    "if df_sample is None:\n",
    "    raise RuntimeError('No rows found in metastore or parquet fallback — cannot compute metrics')\n",
    "\n",
    "df_pd = df_sample.toPandas()\n",
    "print('Pandas sample rows:', len(df_pd))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Prepare numeric features and target\n",
    "# -------------------------------------------------------------------\n",
    "if 'ArrDelay' not in df_pd.columns:\n",
    "    try:\n",
    "        df_pd['ArrDelay'] = pd.to_numeric(df_pd.get('ArrDelay', pd.Series(dtype=float)), errors='coerce')\n",
    "    except Exception:\n",
    "        df_pd['ArrDelay'] = pd.Series([np.nan] * len(df_pd))\n",
    "\n",
    "numeric = df_pd.select_dtypes(include=[np.number]).copy()\n",
    "if 'ArrDelay' not in numeric.columns:\n",
    "    numeric['ArrDelay'] = pd.to_numeric(df_pd['ArrDelay'], errors='coerce')\n",
    "\n",
    "feat_cols = [c for c in numeric.columns if c != 'ArrDelay'][:in_features]\n",
    "if len(feat_cols) < in_features:\n",
    "    raise RuntimeError(f'Not enough numeric features ({len(feat_cols)}) to match model input ({in_features})')\n",
    "\n",
    "X = numeric[feat_cols].astype(np.float32).reset_index(drop=True)\n",
    "y = numeric['ArrDelay'].values.astype(np.float32)\n",
    "\n",
    "if len(X) == 0:\n",
    "    raise RuntimeError('Pandas sample is empty after robust loading — cannot compute metrics')\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Split and scale data\n",
    "# -------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "if isinstance(ckpt, dict) and 'scaler' in ckpt:\n",
    "    scaler = ckpt['scaler']\n",
    "    print('Using scaler loaded from checkpoint (type:', type(scaler), ')')\n",
    "else:\n",
    "    print('No scaler in checkpoint; fitting StandardScaler on training split')\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train.values)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train.values).astype(np.float32)\n",
    "X_test_scaled = scaler.transform(X_test.values).astype(np.float32)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Run inference and compute metrics\n",
    "# -------------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    Xt = torch.from_numpy(X_test_scaled)\n",
    "    preds = model(Xt).cpu().numpy().reshape(-1)\n",
    "\n",
    "rmse = float(np.sqrt(mean_squared_error(y_test, preds)))\n",
    "r2 = float(r2_score(y_test, preds))\n",
    "\n",
    "print('\\nPyTorch baseline metrics:')\n",
    "print(f' RMSE = {rmse:.4f}')\n",
    "print(f' R2   = {r2:.4f}')\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. Compare with previous PySpark baselines\n",
    "# -------------------------------------------------------------------\n",
    "prev_lr_rmse = 11.2483\n",
    "prev_lr_r2 = 0.9056\n",
    "prev_rf_rmse = 17.0213\n",
    "prev_rf_r2 = 0.7837\n",
    "\n",
    "print('\\nPrevious PySpark baselines:')\n",
    "print(f' LinearRegression RMSE={prev_lr_rmse:.4f}, R2={prev_lr_r2:.4f}')\n",
    "print(f' RandomForest     RMSE={prev_rf_rmse:.4f}, R2={prev_rf_r2:.4f}')\n",
    "\n",
    "print('\\nComparison: lower RMSE and higher R² indicate better performance.')\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. Show a few prediction samples\n",
    "# -------------------------------------------------------------------\n",
    "res_df = pd.DataFrame({\n",
    "    'ArrDelay': y_test.flatten(),\n",
    "    'prediction': preds\n",
    "})\n",
    "print('\\nSample predictions:')\n",
    "print(res_df.head(10).to_string(index=False))\n",
    "\n",
    "print('\\nDone — use these metrics to compare with your earlier models.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Models Performance Findings:\n",
    "\n",
    "Among the three models tested, Linear Regression performed best with the lowest RMSE (11.25) and highest R² (0.91), outperforming both the Random Forest and PyTorch models in predicting flight delays.\n",
    "\n",
    "**Reasons:**\n",
    "1. Data is mostly linear in nature – The relationship between predictors (like departure time, day, and weather) and arrival delay might be largely linear, making Linear Regression a natural fit.\n",
    "\n",
    "2. Feature scaling and preprocessing – Since Linear Regression is sensitive to feature scaling (and you standardized the data), it likely benefited more from clean, normalized inputs than the Random Forest or PyTorch models.\n",
    "\n",
    "3. Limited feature interactions or complexity – If the dataset doesn’t have strong nonlinear relationships or high-order interactions, more complex models (like Random Forest or neural nets) can overfit or perform worse.\n",
    "\n",
    "4. Model simplicity and interpretability – Linear Regression has fewer parameters and assumptions, so it generalizes better when the dataset is moderate in size or not highly variable.\n",
    "\n",
    "5. Training configuration differences – The PyTorch model may have been undertrained (e.g., too few epochs, poor learning rate) or lacked sufficient feature scaling, while the Random Forest might not have been optimally tuned for depth or number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load a small sample and prepare tensors\n",
    "We attempt to load `default.flights_2006_transformed` from the Hive metastore first,\n",
    "then fall back to the Parquet path. We keep only numeric columns (except the target `ArrDelay`)\n",
    "and convert a small sample to pandas for training a tiny PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# reuse existing SparkSession if available\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "\n",
    "# Try metastore table first, then parquet fallback\n",
    "tbl_name = 'default.flights_2006_transformed'\n",
    "parquet_path = 'hdfs://namenode:8020/data/parquet/flights_2006_transformed'\n",
    "\n",
    "try:\n",
    "    df_spark = spark.table(tbl_name)\n",
    "    print('Loaded from metastore:', tbl_name)\n",
    "except Exception as e:\n",
    "    print('Metastore load failed, trying parquet fallback:', e)\n",
    "    df_spark = spark.read.parquet(parquet_path)\n",
    "    print('Loaded from parquet:', parquet_path)\n",
    "\n",
    "# take a very small sample that fits in memory (prefer speed over accuracy)\n",
    "sample_fraction = 0.0005  # very small sample fraction for fastest runs\n",
    "df_sample = df_spark.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "print('Sample rows (spark):', df_sample.count())\n",
    "\n",
    "# convert to pandas (careful: keep sample small)\n",
    "df_pd = df_sample.toPandas()\n",
    "print('Sample rows (pandas):', len(df_pd))\n",
    "\n",
    "# Ensure target exists\n",
    "if 'ArrDelay' not in df_pd.columns:\n",
    "    raise ValueError('ArrDelay column not found in sample')\n",
    "\n",
    "# Select numeric features and drop target from features\n",
    "numeric = df_pd.select_dtypes(include=[np.number]).copy()\n",
    "if 'ArrDelay' not in numeric.columns:\n",
    "    # sometimes ArrDelay is object; cast if possible\n",
    "    numeric['ArrDelay'] = pd.to_numeric(df_pd['ArrDelay'], errors='coerce')\n",
    "\n",
    "# Drop rows with NA in ArrDelay\n",
    "numeric = numeric.dropna(subset=['ArrDelay'])\n",
    "\n",
    "# Reduce the feature set to a small number of numeric features to save memory/time\n",
    "top_k = min(8, max(1, numeric.shape[1]-1))  # pick up to 8 features (excluding target)\n",
    "feat_cols = [c for c in numeric.columns if c != 'ArrDelay'][:top_k]\n",
    "X = numeric[feat_cols].copy()\n",
    "y = numeric['ArrDelay'].values.astype(np.float32)\n",
    "\n",
    "# If the pandas sample is still large, downsample to at most 3000 rows for memory safety\n",
    "if len(X) > 3000:\n",
    "    sampled_idx = np.random.RandomState(42).choice(len(X), size=3000, replace=False)\n",
    "    X = X.iloc[sampled_idx].reset_index(drop=True)\n",
    "    y = y[sampled_idx]\n",
    "\n",
    "# If there are zero features (unlikely), raise with helpful message\n",
    "if X.shape[1] == 0:\n",
    "    raise ValueError('No numeric features available after dropping ArrDelay. Check transformed dataset.')\n",
    "\n",
    "print('Features shape:', X.shape)\n",
    "\n",
    "# simple train/test split and scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# keep these for the PyTorch cells below\n",
    "X_train_scaled = X_train_scaled.astype(np.float32)\n",
    "X_test_scaled = X_test_scaled.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "print('Prepared numpy arrays: X_train', X_train_scaled.shape, 'y_train', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small PyTorch model training (CPU)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "Xtr = torch.from_numpy(X_train_scaled)\n",
    "Xte = torch.from_numpy(X_test_scaled)\n",
    "ytr = torch.from_numpy(y_train).unsqueeze(1)\n",
    "yte = torch.from_numpy(y_test).unsqueeze(1)\n",
    "\n",
    "# smaller batch size for faster iterations in constrained environments\n",
    "batch_size = 32\n",
    "train_ds = TensorDataset(Xtr, ytr)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "n_features = Xtr.shape[1]\n",
    "# Very simple single-layer linear model (one linear layer)\n",
    "model = nn.Linear(n_features, 1)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# minimal epochs to shorten run time for a quick baseline\n",
    "epochs = 1\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    count = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb = xb.to(device); yb = yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        running += loss.item() * xb.size(0)\n",
    "        count += xb.size(0)\n",
    "    avg_loss = running / max(1, count)\n",
    "    print(f'Epoch {ep}/{epochs}  train_mse={avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set and save model locally\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(Xte.to(device)).cpu().numpy().reshape(-1)\n",
    "    y_true = y_test.reshape(-1)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, preds))\n",
    "print('Test RMSE:', rmse)\n",
    "\n",
    "# Save model locally (adjust path as needed). To persist to HDFS, copy this file with hadoop/fs commands or Spark write APIs.\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_path = os.path.join('models', 'pytorch_small_regressor.pth')\n",
    "torch.save({'model_state_dict': model.state_dict(), 'scaler': scaler}, model_path)\n",
    "print('Saved model to', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
