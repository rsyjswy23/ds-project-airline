{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train two models using PySpark:\n",
    "- Linear Regression (baseline)\n",
    "- Random Forest Regressor (nonlinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to read from metastore table default.flights_2006_transformed\n",
      "Failed to read table: 'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;'\n",
      "Falling back to parquet path hdfs://namenode:8020/data/parquet/flights_2006_transformed\n",
      "Loaded parquet from hdfs://namenode:8020/data/parquet/flights_2006_transformed\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# reuse active Spark session when possible\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "\n",
    "# load transformed data (table or parquet path)\n",
    "out_trans_table = 'default.flights_2006_transformed'\n",
    "out_trans_path = 'hdfs://namenode:8020/data/parquet/flights_2006_transformed'\n",
    "try:\n",
    "    print('Trying to read from metastore table', out_trans_table)\n",
    "    df = spark.table(out_trans_table)\n",
    "    print('Loaded table', out_trans_table)\n",
    "except Exception as e:\n",
    "    print('Failed to read table:', e)\n",
    "    print('Falling back to parquet path', out_trans_path)\n",
    "    df = spark.read.parquet(out_trans_path)\n",
    "    print('Loaded parquet from', out_trans_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows = 7003802\n",
      "root\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: double (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: double (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- DepHour: integer (nullable = true)\n",
      " |-- ArrHour: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      "\n",
      "+----------+---------+-------+----------+-------+----------+--------+--------+---------+------+-------+--------+-------+-------+----+-----+\n",
      "|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|DepDelay|Distance|Cancelled|TaxiIn|TaxiOut|ArrDelay|DepHour|ArrHour|Year|Month|\n",
      "+----------+---------+-------+----------+-------+----------+--------+--------+---------+------+-------+--------+-------+-------+----+-----+\n",
      "|21        |1        |1310.0 |1315      |1702.0 |1700      |-5.0    |984.0   |0        |5.0   |16.0   |2.0     |13     |17     |2006|8    |\n",
      "|25        |5        |1325.0 |1330      |1407.0 |1409      |-5.0    |109.0   |0        |9.0   |9.0    |-2.0    |13     |14     |2006|8    |\n",
      "|17        |4        |1840.0 |1840      |1942.0 |1941      |0.0     |641.0   |0        |6.0   |21.0   |1.0     |18     |19     |2006|8    |\n",
      "|18        |5        |1906.0 |1846      |2017.0 |1959      |20.0    |258.0   |0        |7.0   |15.0   |18.0    |19     |20     |2006|8    |\n",
      "|24        |4        |2224.0 |2230      |627.0  |640       |-6.0    |2454.0  |0        |6.0   |18.0   |-13.0   |22     |6      |2006|8    |\n",
      "+----------+---------+-------+----------+-------+----------+--------+--------+---------+------+-------+--------+-------+-------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick data checks: row count and schema\n",
    "print('rows =', df.count())\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare features and label\n",
    "We will use `ArrDelay` as the numeric label (regression).\n",
    "Assemble a small set of numeric features (tweak as needed). Keep the pipeline minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered cancelled flights. rows = 7003802\n",
      "Selected features (Year/Cancelled excluded): ['DepDelay', 'Distance', 'TaxiIn', 'TaxiOut', 'DepHour', 'ArrHour', 'DayOfWeek', 'Month']\n",
      "Rows after dropping nulls: 7003802\n",
      "Prepared dataframe with features_vec and label:\n",
      "+---------------------------------------+--------+\n",
      "|features_vec                           |ArrDelay|\n",
      "+---------------------------------------+--------+\n",
      "|[-5.0,984.0,5.0,16.0,13.0,17.0,1.0,8.0]|2.0     |\n",
      "|[-5.0,109.0,9.0,9.0,13.0,14.0,5.0,8.0] |-2.0    |\n",
      "|[0.0,641.0,6.0,21.0,18.0,19.0,4.0,8.0] |1.0     |\n",
      "|[20.0,258.0,7.0,15.0,19.0,20.0,5.0,8.0]|18.0    |\n",
      "|[-6.0,2454.0,6.0,18.0,22.0,6.0,4.0,8.0]|-13.0   |\n",
      "+---------------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# 1) Filter out cancelled flights (keep only Cancelled == 0)\n",
    "if 'Cancelled' in df.columns:\n",
    "    df = df.filter(F.col('Cancelled') == 0)\n",
    "    print('Filtered cancelled flights. rows =', df.count())\n",
    "else:\n",
    "    print('No Cancelled column found; continuing without filter')\n",
    "\n",
    "# 2) Cast common columns to numeric types to avoid ML errors\n",
    "to_cast_double = ['DepDelay','Distance','TaxiIn','TaxiOut','ArrDelay','DepTime','ArrTime']\n",
    "to_cast_int = ['DepHour','ArrHour','DayOfWeek','DayofMonth','Month']\n",
    "for c in to_cast_double:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, F.col(c).cast(DoubleType()))\n",
    "for c in to_cast_int:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(c, F.col(c).cast(IntegerType()))\n",
    "\n",
    "# 3) Choose feature list (explicit) and exclude Year / Cancelled\n",
    "candidate_features = ['DepDelay','Distance','TaxiIn','TaxiOut','DepHour','ArrHour','DayOfWeek','Month']\n",
    "features = [c for c in candidate_features if c in df.columns and c not in ('Year','Cancelled')]\n",
    "print('Selected features (Year/Cancelled excluded):', features)\n",
    "\n",
    "# 4) Ensure label is present\n",
    "label_col = 'ArrDelay'\n",
    "if label_col not in df.columns:\n",
    "    raise ValueError('Label ArrDelay not found in dataframe')\n",
    "\n",
    "# 5) Drop rows with nulls in features or label (simple baseline)\n",
    "use_cols = features + [label_col]\n",
    "df = df.select(*use_cols).na.drop()\n",
    "print('Rows after dropping nulls:', df.count())\n",
    "\n",
    "# Optional: limit for quick dev iterations (uncomment to use)\n",
    "# df = df.limit(200000)\n",
    "\n",
    "# 6) Assemble feature vector for MLlib\n",
    "assembler = VectorAssembler(inputCols=features, outputCol='features_vec')\n",
    "df = assembler.transform(df).select('features_vec', label_col)\n",
    "print('Prepared dataframe with features_vec and label:')\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train / Test split\n",
    "Simple random split: 80% train / 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rows = 5602507 test rows = 1401295\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print('train rows =', train_df.count(), 'test rows =', test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline: Linear Regression (PySpark)\n",
    "Simple linear regression using `features_vec` as input. We'll measure RMSE and R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression training done\n",
      "Linear Regression RMSE: 11.2483, R2: 0.9056\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr = LinearRegression(featuresCol='features_vec', labelCol=label_col, maxIter=50, regParam=0.1)\n",
    "lr_model = lr.fit(train_df)\n",
    "print('Linear Regression training done')\n",
    "\n",
    "# evaluate on test set\n",
    "preds_lr = lr_model.transform(test_df)\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='rmse')\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='r2')\n",
    "rmse_lr = evaluator_rmse.evaluate(preds_lr)\n",
    "r2_lr = evaluator_r2.evaluate(preds_lr)\n",
    "print(f'Linear Regression RMSE: {rmse_lr:.4f}, R2: {r2_lr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE = 11.25 → On average, predicted arrival delays are about ±11 minutes off from the actual delays. Lower RMSE is better.\n",
    "\n",
    "R² = 0.9056 → The model explains ~90.6% of the variance in arrival delays. This is very high for a baseline linear model, meaning the linear relationships captured are strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Nonlinear model: Random Forest Regressor (PySpark)\n",
    "Train a Random Forest Regressor as a stronger nonlinear baseline. Keep hyperparameters small for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest training done\n",
      "Random Forest RMSE: 17.0213, R2: 0.7837\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# small RF for speed (increase numTrees if you want better performance)\n",
    "rf = RandomForestRegressor(featuresCol='features_vec', labelCol=label_col, numTrees=50, maxDepth=8, seed=42)\n",
    "rf_model = rf.fit(train_df)\n",
    "print('Random Forest training done')\n",
    "\n",
    "preds_rf = rf_model.transform(test_df)\n",
    "rmse_rf = evaluator_rmse.evaluate(preds_rf)\n",
    "r2_rf = evaluator_r2.evaluate(preds_rf)\n",
    "print(f'Random Forest RMSE: {rmse_rf:.4f}, R2: {r2_rf:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE = 17.02 → Predictions are, on average, ±17 minutes off. This is higher than Linear Regression, meaning it’s less precise in this setup.\n",
    "\n",
    "R² = 0.7837 → The model explains ~78.4% of the variance. Lower than Linear Regression, so it’s capturing less of the total variance here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite Random Forest being nonlinear and able to capture complex relationships, in this case it underperformed compared to the linear model, and it might mean the features might mostly have linear relationships with `ArrDelay` or Random Forest may require more hyperparameter tuning or more data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
