{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Restart / safety + imports (run first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, corr, isnan\n",
    "import traceback\n",
    "from py4j.protocol import Py4JJavaError\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Create SparkSession with Hive support and explicit metastore URI\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"register_flights_parquet\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:8020/user/hive/warehouse\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Paths and table (change if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To write cleaned Parquet uncomment the write command below\n"
     ]
    }
   ],
   "source": [
    "parquet_path = \"hdfs://namenode:8020/data/parquet/flights_2006\"\n",
    "table_name = \"flights_2006_staged\"\n",
    "\n",
    "OUT_PATH = 'hdfs://namenode:8020/data/parquet/flights_2006_cleaned'\n",
    "print('To write cleaned Parquet uncomment the write command below')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read the external table (or read parquet directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Parquet folder contents:\n"
     ]
    }
   ],
   "source": [
    "parquet_path = \"hdfs://namenode:8020/data/parquet/flights_2006\"\n",
    "\n",
    "print(\"Checking Parquet folder contents:\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.defaultFS\", \"hdfs://namenode:8020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2006|    1|        11|        3|    743|       745|   1024|      1018|           US|      343| N657AW|              281|           273|    223|       6|      -2|   ATL| PHX|    1587|    45|     13|        0|            null|       0|           0|           0|       0|            0|                0|\n",
      "|2006|    1|        11|        3|   1053|      1053|   1313|      1318|           US|      613| N834AW|              260|           265|    214|      -5|       0|   ATL| PHX|    1587|    27|     19|        0|            null|       0|           0|           0|       0|            0|                0|\n",
      "|2006|    1|        11|        3|   1915|      1915|   2110|      2133|           US|      617| N605AW|              235|           258|    220|     -23|       0|   ATL| PHX|    1587|     4|     11|        0|            null|       0|           0|           0|       0|            0|                0|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"hdfs://namenode:8020/data/flights/2006.csv\"  # update path if your CSV is elsewhere\n",
    "parquet_out = \"hdfs://namenode:8020/data/parquet/flights_2006\"\n",
    "\n",
    "df_csv = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(csv_path)\n",
    "df_csv.show(3)\n",
    "df_csv.write.mode(\"overwrite\").option(\"compression\",\"snappy\").parquet(parquet_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: integer (nullable = true)\n",
      " |-- WeatherDelay: integer (nullable = true)\n",
      " |-- NASDelay: integer (nullable = true)\n",
      " |-- SecurityDelay: integer (nullable = true)\n",
      " |-- LateAircraftDelay: integer (nullable = true)\n",
      "\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2006|5    |4         |4        |1943   |1939      |2152   |2144      |CO           |159      |N79279 |189              |185           |163    |8       |4       |DCA   |IAH |1208    |4     |22     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|5    |26        |5        |1236   |1240      |1334   |1335      |CO           |1758     |N14628 |58               |55            |32     |-1      |-4      |AUS   |IAH |140     |14    |12     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|5    |2         |2        |1837   |1845      |2104   |2105      |CO           |1070     |N14342 |147              |140           |124    |-1      |-8      |MCO   |CLE |895     |8     |15     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|5    |6         |6        |1110   |1100      |1244   |1213      |CO           |1671     |N69333 |94               |73            |54     |31      |10      |MFE   |IAH |316     |6     |34     |0        |null            |0       |0           |0           |21      |0            |10               |\n",
      "|2006|5    |26        |5        |1344   |1315      |1625   |1612      |CO           |17       |N33132 |341              |357           |314    |13      |29      |EWR   |LAX |2454    |7     |20     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"mergeSchema\",\"true\").parquet(\"hdfs://namenode:8020/data/parquet/flights_2006\")\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found via SHOW TABLES IN default LIKE: False\n",
      "Found via catalog.listTables: False\n",
      "Final table_exists: False\n"
     ]
    }
   ],
   "source": [
    "# Cell: check for Hive table existence (robust across Spark versions)\n",
    "table_name = 'flights_2006_staged'\n",
    "\n",
    "# 1) SQL check\n",
    "try:\n",
    "    tbls = spark.sql(f\"SHOW TABLES IN default LIKE '{table_name}'\")\n",
    "    exists_sql = tbls.count() > 0\n",
    "    print('Found via SHOW TABLES IN default LIKE:', exists_sql)\n",
    "except Exception as e:\n",
    "    print('SHOW TABLES failed:', e); exists_sql = False\n",
    "\n",
    "# 2) catalog.listTables fallback\n",
    "exists_catalog = False\n",
    "try:\n",
    "    try:\n",
    "        tables = spark.catalog.listTables('default')\n",
    "    except TypeError:\n",
    "        tables = spark.catalog.listTables()\n",
    "    names = [t.name for t in tables]\n",
    "    exists_catalog = table_name in names\n",
    "    print('Found via catalog.listTables:', exists_catalog)\n",
    "except Exception as e:\n",
    "    print('catalog.listTables failed:', e)\n",
    "\n",
    "table_exists = exists_sql or exists_catalog\n",
    "print('Final table_exists:', table_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found — creating EXTERNAL table using current DataFrame schema (df must exist)\n",
      "Running CREATE EXTERNAL TABLE...\n",
      "MSCK REPAIR TABLE returned (may be OK): 'Operation not allowed: MSCK REPAIR TABLE only works on partitioned tables: `default`.`flights_2006_staged`;'\n",
      "CREATE TABLE executed (or table already existed).\n"
     ]
    }
   ],
   "source": [
    "# Cell: create EXTERNAL Hive table pointing to Parquet if missing\n",
    "parquet_path = 'hdfs://namenode:8020/data/parquet/flights_2006'\n",
    "table_name = 'flights_2006_staged'\n",
    "\n",
    "if not ( 'table_exists' in globals() and table_exists ):\n",
    "    print('Table not found — creating EXTERNAL table using current DataFrame schema (df must exist)')\n",
    "    try:\n",
    "        df\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"DataFrame 'df' not found — run the loader cell first.\")\n",
    "\n",
    "    # simple mapping from Spark dtypes to Hive types\n",
    "    type_map = {\n",
    "        'int':'INT','bigint':'BIGINT','string':'STRING','double':'DOUBLE','float':'FLOAT',\n",
    "        'boolean':'BOOLEAN','tinyint':'TINYINT','smallint':'SMALLINT','decimal':'DECIMAL'\n",
    "    }\n",
    "\n",
    "    cols = []\n",
    "    for name,dtype in df.dtypes:\n",
    "        hive_type = type_map.get(dtype.lower(), 'STRING')\n",
    "        cols.append(f\"`{name}` {hive_type}\")\n",
    "    cols_ddl = ',\\n  '.join(cols)\n",
    "\n",
    "    create_stmt = f\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS default.{table_name} (\n",
    "  {cols_ddl}\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '{parquet_path}'\"\"\"\n",
    "\n",
    "    print('Running CREATE EXTERNAL TABLE...')\n",
    "    try:\n",
    "        spark.sql(create_stmt)\n",
    "        try:\n",
    "            spark.sql(f\"MSCK REPAIR TABLE default.{table_name}\")\n",
    "        except Exception as e_rep:\n",
    "            print('MSCK REPAIR TABLE returned (may be OK):', e_rep)\n",
    "        try:\n",
    "            spark.catalog.refreshTable(table_name)\n",
    "        except Exception:\n",
    "            try:\n",
    "                spark.catalog.refreshTable(f'default.{table_name}')\n",
    "            except Exception as e_ref:\n",
    "                print('catalog.refreshTable failed (non-fatal):', e_ref)\n",
    "        print('CREATE TABLE executed (or table already existed).')\n",
    "    except Exception:\n",
    "        import traceback; traceback.print_exc()\n",
    "        raise\n",
    "else:\n",
    "    print('Table already exists — skipping create.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW TABLES (default):\n",
      "+--------+-------------------+-----------+\n",
      "|database|tableName          |isTemporary|\n",
      "+--------+-------------------+-----------+\n",
      "|default |flights_2006_staged|false      |\n",
      "+--------+-------------------+-----------+\n",
      "\n",
      "\n",
      "Selecting a small sample from the Hive table:\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2006|5    |4         |4        |1943   |1939      |2152   |2144      |CO           |159      |N79279 |189              |185           |163    |8       |4       |DCA   |IAH |1208    |4     |22     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|5    |26        |5        |1236   |1240      |1334   |1335      |CO           |1758     |N14628 |58               |55            |32     |-1      |-4      |AUS   |IAH |140     |14    |12     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|5    |2         |2        |1837   |1845      |2104   |2105      |CO           |1070     |N14342 |147              |140           |124    |-1      |-8      |MCO   |CLE |895     |8     |15     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|5    |6         |6        |1110   |1100      |1244   |1213      |CO           |1671     |N69333 |94               |73            |54     |31      |10      |MFE   |IAH |316     |6     |34     |0        |null            |0       |0           |0           |21      |0            |10               |\n",
      "|2006|5    |26        |5        |1344   |1315      |1625   |1612      |CO           |17       |N33132 |341              |357           |314    |13      |29      |EWR   |LAX |2454    |7     |20     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell: verify table is readable (sample and optional count)\n",
    "table_name = 'flights_2006_staged'\n",
    "\n",
    "print('SHOW TABLES (default):')\n",
    "try:\n",
    "    spark.sql('SHOW TABLES IN default').show(truncate=False)\n",
    "except Exception as e:\n",
    "    print('SHOW TABLES failed:', e)\n",
    "\n",
    "if spark.sql(f\"SHOW TABLES IN default LIKE '{table_name}'\").count() > 0:\n",
    "    print('\\nSelecting a small sample from the Hive table:')\n",
    "    try:\n",
    "        spark.sql(f\"SELECT * FROM default.{table_name} LIMIT 5\").show(truncate=False)\n",
    "    except Exception:\n",
    "        import traceback; traceback.print_exc()\n",
    "    # Optional: count rows (may be slow)\n",
    "    # spark.sql(f\"SELECT COUNT(*) AS cnt FROM default.{table_name}\").show()\n",
    "else:\n",
    "    print('Table not present after create attempt; check create cell output for errors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    cnt|\n",
      "+-------+\n",
      "|7141922|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) AS cnt FROM default.flights_2006_staged\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Handle missing values. (Remove Rows with null or NaN values in the target column (ArrDelay), ensuring the reliability of the target variable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before cleaning: 7,141,922\n",
      "Rows after cleaning: 7,003,802\n",
      "Rows removed: 138,120\n",
      "\n",
      "clean_df schema:\n",
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: integer (nullable = true)\n",
      " |-- WeatherDelay: integer (nullable = true)\n",
      " |-- NASDelay: integer (nullable = true)\n",
      " |-- SecurityDelay: integer (nullable = true)\n",
      " |-- LateAircraftDelay: integer (nullable = true)\n",
      "\n",
      "\n",
      "clean_df sample rows:\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2006|5    |4         |4        |1943   |1939      |2152   |2144      |CO           |159      |N79279 |189              |185           |163    |8.0     |4       |DCA   |IAH |1208    |4     |22     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|5    |26        |5        |1236   |1240      |1334   |1335      |CO           |1758     |N14628 |58               |55            |32     |-1.0    |-4      |AUS   |IAH |140     |14    |12     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|5    |2         |2        |1837   |1845      |2104   |2105      |CO           |1070     |N14342 |147              |140           |124    |-1.0    |-8      |MCO   |CLE |895     |8     |15     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|5    |6         |6        |1110   |1100      |1244   |1213      |CO           |1671     |N69333 |94               |73            |54     |31.0    |10      |MFE   |IAH |316     |6     |34     |0        |null            |0       |0           |0           |21      |0            |10               |\n",
      "|2006|5    |26        |5        |1344   |1315      |1625   |1612      |CO           |17       |N33132 |341              |357           |314    |13.0    |29      |EWR   |LAX |2454    |7     |20     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    raise RuntimeError(\"DataFrame 'df' not found — run the loader cell first\")\n",
    "\n",
    "# Cast ArrDelay to double; non-numeric values become null\n",
    "df_cast = df.withColumn('ArrDelay', F.col('ArrDelay').cast(DoubleType()))\n",
    "\n",
    "# Counts before/after filtering\n",
    "count_before = df_cast.count()\n",
    "clean_df = df_cast.filter(F.col('ArrDelay').isNotNull() & (~F.isnan(F.col('ArrDelay'))))\n",
    "count_after = clean_df.count()\n",
    "\n",
    "print(f'Rows before cleaning: {count_before:,}')\n",
    "print(f'Rows after cleaning: {count_after:,}')\n",
    "print(f'Rows removed: {count_before - count_after:,}')\n",
    "\n",
    "# Quick sanity: show schema and a few rows from clean_df\n",
    "print('\\nclean_df schema:')\n",
    "clean_df.printSchema()\n",
    "print('\\nclean_df sample rows:')\n",
    "clean_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Numeric summary for ArrDelay (central tendency & spread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.003802e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.682840e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stddev</th>\n",
       "      <td>3.657647e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.920000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q1 (1%)</th>\n",
       "      <td>-5.920000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q5 (5%)</th>\n",
       "      <td>-1.900000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q1 (25%)</th>\n",
       "      <td>-9.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median (50%)</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q3 (75%)</th>\n",
       "      <td>1.400000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IQR (q3-q1)</th>\n",
       "      <td>2.300000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q95</th>\n",
       "      <td>7.700000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q99</th>\n",
       "      <td>1.779000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.779000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     value\n",
       "metric                    \n",
       "count         7.003802e+06\n",
       "mean          8.682840e+00\n",
       "stddev        3.657647e+01\n",
       "min          -5.920000e+02\n",
       "q1 (1%)      -5.920000e+02\n",
       "q5 (5%)      -1.900000e+01\n",
       "q1 (25%)     -9.000000e+00\n",
       "median (50%)  0.000000e+00\n",
       "q3 (75%)      1.400000e+01\n",
       "IQR (q3-q1)   2.300000e+01\n",
       "q95           7.700000e+01\n",
       "q99           1.779000e+03\n",
       "max           1.779000e+03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute basic aggregates (fast)\n",
    "agg_row = clean_df.select(\n",
    "    F.count(\"ArrDelay\").alias(\"count\"),\n",
    "    F.mean(\"ArrDelay\").alias(\"mean\"),\n",
    "    F.stddev(\"ArrDelay\").alias(\"stddev\"),\n",
    "    F.min(\"ArrDelay\").alias(\"min\"),\n",
    "    F.max(\"ArrDelay\").alias(\"max\")\n",
    ").collect()[0]\n",
    "\n",
    "# approximate quantiles (fast, avoids full shuffle)\n",
    "q_probs = [0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]\n",
    "q_vals = clean_df.approxQuantile(\"ArrDelay\", q_probs, 0.01)\n",
    "q_map = {f\"q{int(p*100)}\": v for p, v in zip(q_probs, q_vals)}\n",
    "\n",
    "# compute IQR\n",
    "iqr = q_map[\"q75\"] - q_map[\"q25\"]\n",
    "\n",
    "# assemble tidy table as pandas DataFrame (safe: small)\n",
    "rows = [\n",
    "    (\"count\", int(agg_row[\"count\"])),\n",
    "    (\"mean\", float(agg_row[\"mean\"]) if agg_row[\"mean\"] is not None else None),\n",
    "    (\"stddev\", float(agg_row[\"stddev\"]) if agg_row[\"stddev\"] is not None else None),\n",
    "    (\"min\", float(agg_row[\"min\"]) if agg_row[\"min\"] is not None else None),\n",
    "    (\"max\", float(agg_row[\"max\"]) if agg_row[\"max\"] is not None else None),\n",
    "    (\"q1 (25%)\", q_map[\"q25\"]),\n",
    "    (\"median (50%)\", q_map[\"q50\"]),\n",
    "    (\"q3 (75%)\", q_map[\"q75\"]),\n",
    "    (\"IQR (q3-q1)\", iqr),\n",
    "    (\"q95\", q_map[\"q95\"]),\n",
    "    (\"q99\", q_map[\"q99\"]),\n",
    "    (\"q5 (5%)\", q_map[\"q5\"]),\n",
    "    (\"q1 (1%)\", q_map[\"q1\"])\n",
    "]\n",
    "\n",
    "# reorder for readability (preferred order)\n",
    "order = [\"count\",\"mean\",\"stddev\",\"min\",\"q1 (1%)\",\"q5 (5%)\",\"q1 (25%)\",\"median (50%)\",\"q3 (75%)\",\"IQR (q3-q1)\",\"q95\",\"q99\",\"max\"]\n",
    "# build df\n",
    "df_stats = pd.DataFrame(rows, columns=[\"metric\",\"value\"]).set_index(\"metric\").reindex(order)\n",
    "\n",
    "# display nicely in notebook\n",
    "from IPython.display import display\n",
    "display(df_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Distribution / buckets for ArrDelay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delay_bucket</th>\n",
       "      <th>n</th>\n",
       "      <th>pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-15,0]</td>\n",
       "      <td>2957589</td>\n",
       "      <td>42.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;= 15</td>\n",
       "      <td>1615537</td>\n",
       "      <td>23.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0,15)</td>\n",
       "      <td>1581829</td>\n",
       "      <td>22.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;= -15</td>\n",
       "      <td>848847</td>\n",
       "      <td>12.120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  delay_bucket        n     pct\n",
       "0      (-15,0]  2957589  42.228\n",
       "1        >= 15  1615537  23.067\n",
       "2       (0,15)  1581829  22.585\n",
       "3       <= -15   848847  12.120"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bucket ArrDelay into categories and show counts & percentages\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "buckets = clean_df.withColumn(\"delay_bucket\",\n",
    "    F.when(F.col(\"ArrDelay\") <= -15, \"<= -15\")\n",
    "     .when((F.col(\"ArrDelay\") > -15) & (F.col(\"ArrDelay\") <= 0), \"(-15,0]\")\n",
    "     .when((F.col(\"ArrDelay\") > 0) & (F.col(\"ArrDelay\") < 15), \"(0,15)\")\n",
    "     .otherwise(\">= 15\")\n",
    ")\n",
    "\n",
    "total = buckets.count()\n",
    "bucket_summary = (buckets.groupBy(\"delay_bucket\")\n",
    "                         .agg(F.count(\"*\").alias(\"n\"))\n",
    "                         .withColumn(\"pct\", F.round(F.col(\"n\") / total * 100, 3))\n",
    "                         .orderBy(F.desc(\"n\")))\n",
    "bucket_summary.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Target class balance & temporal breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delay_bucket</th>\n",
       "      <th>n</th>\n",
       "      <th>pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-15,0]</td>\n",
       "      <td>2957589</td>\n",
       "      <td>42.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;= 15</td>\n",
       "      <td>1615537</td>\n",
       "      <td>23.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0,15)</td>\n",
       "      <td>1581829</td>\n",
       "      <td>22.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;= -15</td>\n",
       "      <td>848847</td>\n",
       "      <td>12.120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  delay_bucket        n     pct\n",
       "0      (-15,0]  2957589  42.228\n",
       "1        >= 15  1615537  23.067\n",
       "2       (0,15)  1581829  22.585\n",
       "3       <= -15   848847  12.120"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bucket ArrDelay into categories and show counts & percentages\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "buckets = clean_df.withColumn(\"delay_bucket\",\n",
    "    F.when(F.col(\"ArrDelay\") <= -15, \"<= -15\")\n",
    "     .when((F.col(\"ArrDelay\") > -15) & (F.col(\"ArrDelay\") <= 0), \"(-15,0]\")\n",
    "     .when((F.col(\"ArrDelay\") > 0) & (F.col(\"ArrDelay\") < 15), \"(0,15)\")\n",
    "     .otherwise(\">= 15\")\n",
    ")\n",
    "\n",
    "total = buckets.count()\n",
    "bucket_summary = (buckets.groupBy(\"delay_bucket\")\n",
    "                         .agg(F.count(\"*\").alias(\"n\"))\n",
    "                         .withColumn(\"pct\", F.round(F.col(\"n\") / total * 100, 3))\n",
    "                         .orderBy(F.desc(\"n\")))\n",
    "bucket_summary.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Top origins/destinations & average delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>delayed</th>\n",
       "      <th>pct_delayed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7003802</td>\n",
       "      <td>1615537</td>\n",
       "      <td>23.067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     total  delayed  pct_delayed\n",
       "0  7003802  1615537       23.067"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top months by delay rate:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>n</th>\n",
       "      <th>delayed</th>\n",
       "      <th>pct_delayed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>585271</td>\n",
       "      <td>157128</td>\n",
       "      <td>26.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>586418</td>\n",
       "      <td>150683</td>\n",
       "      <td>25.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>599086</td>\n",
       "      <td>153067</td>\n",
       "      <td>25.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>608631</td>\n",
       "      <td>150771</td>\n",
       "      <td>24.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>518645</td>\n",
       "      <td>118610</td>\n",
       "      <td>22.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>617379</td>\n",
       "      <td>140784</td>\n",
       "      <td>22.803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Month       n  delayed  pct_delayed\n",
       "0     12  585271   157128       26.847\n",
       "1      6  586418   150683       25.695\n",
       "2     10  599086   153067       25.550\n",
       "3      7  608631   150771       24.772\n",
       "4      2  518645   118610       22.869\n",
       "5      8  617379   140784       22.803"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top days-of-week by delay rate:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>n</th>\n",
       "      <th>delayed</th>\n",
       "      <th>pct_delayed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1032497</td>\n",
       "      <td>279964</td>\n",
       "      <td>27.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1028443</td>\n",
       "      <td>267915</td>\n",
       "      <td>26.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1027988</td>\n",
       "      <td>236539</td>\n",
       "      <td>23.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>993653</td>\n",
       "      <td>226170</td>\n",
       "      <td>22.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1022327</td>\n",
       "      <td>225876</td>\n",
       "      <td>22.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1012483</td>\n",
       "      <td>206593</td>\n",
       "      <td>20.405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>886411</td>\n",
       "      <td>172480</td>\n",
       "      <td>19.458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DayOfWeek        n  delayed  pct_delayed\n",
       "0          5  1032497   279964       27.115\n",
       "1          4  1028443   267915       26.051\n",
       "2          1  1027988   236539       23.010\n",
       "3          7   993653   226170       22.761\n",
       "4          3  1022327   225876       22.094\n",
       "5          2  1012483   206593       20.405\n",
       "6          6   886411   172480       19.458"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Overall delay rate and breakdown by Month and DayOfWeek\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# define delayed flag\n",
    "flagged = clean_df.withColumn(\"is_delayed\", (F.col(\"ArrDelay\") >= 15).cast(\"int\"))\n",
    "\n",
    "overall = flagged.agg(\n",
    "    F.count(\"*\").alias(\"total\"),\n",
    "    F.sum(\"is_delayed\").alias(\"delayed\")\n",
    ").withColumn(\"pct_delayed\", F.round(F.col(\"delayed\") / F.col(\"total\") * 100, 3))\n",
    "\n",
    "by_month = (flagged.groupBy(\"Month\")\n",
    "                  .agg(\n",
    "                      F.count(\"*\").alias(\"n\"),\n",
    "                      F.sum(\"is_delayed\").alias(\"delayed\")\n",
    "                  )\n",
    "                  .withColumn(\"pct_delayed\", F.round(F.col(\"delayed\") / F.col(\"n\") * 100, 3))\n",
    "                  .orderBy(F.desc(\"pct_delayed\"))\n",
    "            )\n",
    "\n",
    "by_dow = (flagged.groupBy(\"DayOfWeek\")\n",
    "                .agg(F.count(\"*\").alias(\"n\"), F.sum(\"is_delayed\").alias(\"delayed\"))\n",
    "                .withColumn(\"pct_delayed\", F.round(F.col(\"delayed\") / F.col(\"n\") * 100, 3))\n",
    "                .orderBy(F.desc(\"pct_delayed\"))\n",
    "         )\n",
    "\n",
    "print(\"Overall:\")\n",
    "display(overall.toPandas())\n",
    "print(\"Top months by delay rate:\")\n",
    "display(by_month.limit(6).toPandas())\n",
    "print(\"Top days-of-week by delay rate:\")\n",
    "display(by_dow.limit(7).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the day of the week really matters for flight delays. \n",
    "* Fridays have the most delays at around 27%, while Saturdays are much lower at about 19%. This makes DayOfWeek a simple but strong predictor: a flight on Friday is much more likely to be delayed than the same flight on Saturday. It’s definitely worth including in a model, maybe along with factors like departure time or route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.cancelAllJobs()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
