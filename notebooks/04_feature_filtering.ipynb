{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.kernel.restart()"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "Javascript(\"Jupyter.notebook.kernel.restart()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col, mean, stddev, corr, isnan\n",
    "import traceback\n",
    "from py4j.protocol import Py4JJavaError\n",
    "import pandas as pd\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from scripts.load_and_clean import load_and_clean\n",
    "from scripts.select_features import select_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load `clean_df` via scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet path not found: hdfs://namenode:8020/data/parquet/flights_2006_cleaned\n",
      "Parquet path not found: hdfs://namenode:8020/data/parquet/flights_2006\n",
      "Parquet path not found: hdfs://namenode:8020/data/parquet/flights_2006_features\n",
      "Loading raw CSV fallback: hdfs://namenode:8020/data/flights/2006.csv\n",
      "   Year  Month  DayofMonth  DayOfWeek DepTime  CRSDepTime ArrTime  CRSArrTime  \\\n",
      "0  2006      1          11          3     743         745    1024        1018   \n",
      "1  2006      1          11          3    1053        1053    1313        1318   \n",
      "2  2006      1          11          3    1915        1915    2110        2133   \n",
      "3  2006      1          11          3    1753        1755    1925        1933   \n",
      "4  2006      1          11          3     824         832    1015        1015   \n",
      "\n",
      "  UniqueCarrier  FlightNum  ... TaxiIn TaxiOut Cancelled CancellationCode  \\\n",
      "0            US        343  ...     45      13         0             None   \n",
      "1            US        613  ...     27      19         0             None   \n",
      "2            US        617  ...      4      11         0             None   \n",
      "3            US        300  ...     16      10         0             None   \n",
      "4            US        765  ...     27      12         0             None   \n",
      "\n",
      "   Diverted CarrierDelay WeatherDelay NASDelay  SecurityDelay  \\\n",
      "0         0            0            0        0              0   \n",
      "1         0            0            0        0              0   \n",
      "2         0            0            0        0              0   \n",
      "3         0            0            0        0              0   \n",
      "4         0            0            0        0              0   \n",
      "\n",
      "   LateAircraftDelay  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "rows before: 7,141,922 | rows after (clean): 7,003,802 | removed: 138,120\n",
      "Loaded clean_df rows: 7003802\n",
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: integer (nullable = true)\n",
      " |-- WeatherDelay: integer (nullable = true)\n",
      " |-- NASDelay: integer (nullable = true)\n",
      " |-- SecurityDelay: integer (nullable = true)\n",
      " |-- LateAircraftDelay: integer (nullable = true)\n",
      "\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2006|1    |11        |3        |743    |745       |1024   |1018      |US           |343      |N657AW |281              |273           |223    |6.0     |-2      |ATL   |PHX |1587    |45    |13     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |1053   |1053      |1313   |1318      |US           |613      |N834AW |260              |265           |214    |-5.0    |0       |ATL   |PHX |1587    |27    |19     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |1915   |1915      |2110   |2133      |US           |617      |N605AW |235              |258           |220    |-23.0   |0       |ATL   |PHX |1587    |4     |11     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |1753   |1755      |1925   |1933      |US           |300      |N312AW |152              |158           |126    |-8.0    |-2      |AUS   |PHX |872     |16    |10     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |824    |832       |1015   |1015      |US           |765      |N309AW |171              |163           |132    |0.0     |-8      |AUS   |PHX |872     |27    |12     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load & clean (use the kernel SparkSession)\n",
    "spark, clean_df = load_and_clean()\n",
    "\n",
    "print(\"Loaded clean_df rows:\", clean_df.count())\n",
    "clean_df.printSchema()\n",
    "clean_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "has count attr: True\n",
      "count callable: True\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "rows: 7003802\n"
     ]
    }
   ],
   "source": [
    "# clean target variable\n",
    "# Diagnostics - run in your notebook cell\n",
    "print(type(clean_df))\n",
    "print(\"has count attr:\", hasattr(clean_df, 'count'))\n",
    "print(\"count callable:\", callable(getattr(clean_df, 'count', None)))\n",
    "print(type(clean_df))\n",
    "print(\"rows:\", clean_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "rows: 7003802\n",
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: integer (nullable = true)\n",
      " |-- WeatherDelay: integer (nullable = true)\n",
      " |-- NASDelay: integer (nullable = true)\n",
      " |-- SecurityDelay: integer (nullable = true)\n",
      " |-- LateAircraftDelay: integer (nullable = true)\n",
      "\n",
      "+-------+--------+\n",
      "|summary|ArrDelay|\n",
      "+-------+--------+\n",
      "|    min|  -592.0|\n",
      "|    25%|    -9.0|\n",
      "|    50%|    -1.0|\n",
      "|    75%|    13.0|\n",
      "|    max|  1779.0|\n",
      "+-------+--------+\n",
      "\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2006|1    |11        |3        |743    |745       |1024   |1018      |US           |343      |N657AW |281              |273           |223    |6.0     |-2      |ATL   |PHX |1587    |45    |13     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |1053   |1053      |1313   |1318      |US           |613      |N834AW |260              |265           |214    |-5.0    |0       |ATL   |PHX |1587    |27    |19     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |1915   |1915      |2110   |2133      |US           |617      |N605AW |235              |258           |220    |-23.0   |0       |ATL   |PHX |1587    |4     |11     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |1753   |1755      |1925   |1933      |US           |300      |N312AW |152              |158           |126    |-8.0    |-2      |AUS   |PHX |872     |16    |10     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |824    |832       |1015   |1015      |US           |765      |N309AW |171              |163           |132    |0.0     |-8      |AUS   |PHX |872     |27    |12     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify counts & schema\n",
    "print(\"type:\", type(clean_df))\n",
    "print(\"rows:\", clean_df.count())\n",
    "clean_df.printSchema()\n",
    "\n",
    "# ArrDelay summary (spark summary)\n",
    "clean_df.select('ArrDelay').summary('min','25%','50%','75%','max').show()\n",
    "\n",
    "# quick sample\n",
    "clean_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection: select relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_df type: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "rows: 7003802\n",
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      "\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+--------+--------+---------+------+-------+--------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|DepDelay|Distance|Cancelled|TaxiIn|TaxiOut|ArrDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+--------+--------+---------+------+-------+--------+\n",
      "|2006|1    |11        |3        |743    |745       |1024   |1018      |-2.0    |1587.0  |0        |45.0  |13.0   |6.0     |\n",
      "|2006|1    |11        |3        |1053   |1053      |1313   |1318      |0.0     |1587.0  |0        |27.0  |19.0   |-5.0    |\n",
      "|2006|1    |11        |3        |1915   |1915      |2110   |2133      |0.0     |1587.0  |0        |4.0   |11.0   |-23.0   |\n",
      "|2006|1    |11        |3        |1753   |1755      |1925   |1933      |-2.0    |872.0   |0        |16.0  |10.0   |-8.0    |\n",
      "|2006|1    |11        |3        |824    |832       |1015   |1015      |-8.0    |872.0   |0        |27.0  |12.0   |0.0     |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+--------+--------+---------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Missing values report:\n",
      "        column  n_missing  pct_missing\n",
      "0         Year          0          0.0\n",
      "1        Month          0          0.0\n",
      "2   DayofMonth          0          0.0\n",
      "3    DayOfWeek          0          0.0\n",
      "4      DepTime          0          0.0\n",
      "5   CRSDepTime          0          0.0\n",
      "6      ArrTime          0          0.0\n",
      "7   CRSArrTime          0          0.0\n",
      "8     DepDelay          0          0.0\n",
      "9     Distance          0          0.0\n",
      "10   Cancelled          0          0.0\n",
      "11      TaxiIn          0          0.0\n",
      "12     TaxiOut          0          0.0\n",
      "13    ArrDelay          0          0.0\n"
     ]
    }
   ],
   "source": [
    "# Minimal explicit desired columns using names from your schema\n",
    "keep_cols = [\n",
    "    'Year', 'Month', 'DayofMonth', 'DayOfWeek',\n",
    "    'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime',\n",
    "    'DepDelay', 'Distance', 'Cancelled', 'TaxiIn', 'TaxiOut',\n",
    "    'ArrDelay'\n",
    "]\n",
    "\n",
    "# Only keep columns that actually exist (safe)\n",
    "existing = [c for c in keep_cols if c in clean_df.columns]\n",
    "missing = [c for c in keep_cols if c not in clean_df.columns]\n",
    "if missing:\n",
    "    print(\"Warning — these requested columns are missing and will be skipped:\", missing)\n",
    "\n",
    "# Build select expressions and cast to sensible types\n",
    "exprs = []\n",
    "for c in existing:\n",
    "    if c in ('Year', 'Month', 'DayofMonth', 'DayOfWeek', 'Cancelled'):\n",
    "        exprs.append(F.col(c).cast(T.IntegerType()).alias(c))\n",
    "    elif c in ('DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime'):\n",
    "        # times often stored as hhmm strings -> cast to int (may produce null for empty strings)\n",
    "        exprs.append(F.col(c).cast(T.IntegerType()).alias(c))\n",
    "    elif c in ('DepDelay', 'ArrDelay', 'TaxiIn', 'TaxiOut', 'Distance'):\n",
    "        exprs.append(F.col(c).cast(T.DoubleType()).alias(c))\n",
    "    else:\n",
    "        exprs.append(F.col(c))\n",
    "\n",
    "# Create features_df\n",
    "features_df = clean_df.select(*exprs).persist()\n",
    "\n",
    "# Materialize checks\n",
    "print(\"features_df type:\", type(features_df))\n",
    "print(\"rows:\", features_df.count())\n",
    "features_df.printSchema()\n",
    "features_df.show(5, truncate=False)\n",
    "\n",
    "# Quick missingness report (safe: catches non-numeric isnan usage)\n",
    "total = features_df.count()\n",
    "miss = []\n",
    "for col in features_df.columns:\n",
    "    col_expr = F.col(col)\n",
    "    try:\n",
    "        nnull = features_df.filter(col_expr.isNull() | F.isnan(col_expr) | (col_expr == '')).count()\n",
    "    except Exception:\n",
    "        nnull = features_df.filter(col_expr.isNull() | (col_expr == '')).count()\n",
    "    miss.append((col, int(nnull), round(nnull / total * 100, 3)))\n",
    "\n",
    "miss_df = pd.DataFrame(miss, columns=['column', 'n_missing', 'pct_missing']).sort_values('pct_missing', ascending=False)\n",
    "print(\"\\nMissing values report:\")\n",
    "print(miss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Writes features_df to HDFS as partitioned Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote features_df to hdfs://namenode:8020/data/parquet/flights_2006_features\n"
     ]
    }
   ],
   "source": [
    "# Path where features Parquet will be written on HDFS\n",
    "out_path = 'hdfs://namenode:8020/data/parquet/flights_2006_features'\n",
    "try:\n",
    "    # write partitioned by Year and Month with snappy compression\n",
    "    features_df.write.mode('overwrite').partitionBy('Year','Month').option('compression','snappy').parquet(out_path)\n",
    "    print('Wrote features_df to', out_path)\n",
    "except Exception as e:\n",
    "    print('Failed to write features to HDFS:', e)\n",
    "    # re-raise so the notebook shows the full traceback if desired\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Register a Hive external table points to Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered table default.flights_2006_features using PARQUET OPTIONS (no data rewrite)\n"
     ]
    }
   ],
   "source": [
    "# Try to register a Hive/Metastore table that points at the existing Parquet data.\n",
    "# 1) Non-destructive: ask Spark to create a table that uses the Parquet files (no data rewrite)\n",
    "try:\n",
    "    spark.sql(\n",
    "        f\"CREATE TABLE IF NOT EXISTS default.flights_2006_features USING PARQUET OPTIONS (path '{out_path}')\"\n",
    "    )\n",
    "    spark.sql('MSCK REPAIR TABLE default.flights_2006_features')\n",
    "    print('Registered table default.flights_2006_features using PARQUET OPTIONS (no data rewrite)')\n",
    "except Exception as e1:\n",
    "    print('Primary registration (USING PARQUET OPTIONS) failed:', e1)\n",
    "    print('Falling back to read+saveAsTable which will rewrite the table metadata (and may rewrite files).')\n",
    "    try:\n",
    "        df = spark.read.parquet(out_path)\n",
    "        # saveAsTable with explicit path ensures metastore entry points to the existing location\n",
    "        df.write.mode('overwrite').option('path', out_path).partitionBy('Year','Month').format('parquet').saveAsTable('default.flights_2006_features')\n",
    "        print('Wrote and registered default.flights_2006_features via saveAsTable')\n",
    "    except Exception as e2:\n",
    "        print('Fallback registration also failed:', e2)\n",
    "        print('You can still read the Parquet directly in Notebook 05 with spark.read.parquet(out_path)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
