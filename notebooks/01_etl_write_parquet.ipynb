{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verify Spark → HDFS connectivity and to inspect the schema / a few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema:\n",
      "root\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- DayofMonth: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: string (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: string (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: string (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: string (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: string (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n",
      "show 10 rows:\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2006|1    |11        |3        |743    |745       |1024   |1018      |US           |343      |N657AW |281              |273           |223    |6       |-2      |ATL   |PHX |1587    |45    |13     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |1053   |1053      |1313   |1318      |US           |613      |N834AW |260              |265           |214    |-5      |0       |ATL   |PHX |1587    |27    |19     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |1915   |1915      |2110   |2133      |US           |617      |N605AW |235              |258           |220    |-23     |0       |ATL   |PHX |1587    |4     |11     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |1753   |1755      |1925   |1933      |US           |300      |N312AW |152              |158           |126    |-8      |-2      |AUS   |PHX |872     |16    |10     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |824    |832       |1015   |1015      |US           |765      |N309AW |171              |163           |132    |0       |-8      |AUS   |PHX |872     |27    |12     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |627    |630       |834    |832       |US           |295      |N733UW |127              |122           |108    |2       |-3      |BDL   |CLT |644     |6     |13     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |825    |820       |1041   |1021      |US           |349      |N177UW |136              |121           |111    |20      |5       |BDL   |CLT |644     |4     |21     |0        |null            |0       |0           |0           |20      |0            |0                |\n",
      "|2006|1    |11        |3        |942    |945       |1155   |1148      |US           |356      |N404US |133              |123           |121    |7       |-3      |BDL   |CLT |644     |4     |8      |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |1239   |1245      |1438   |1445      |US           |775      |N722UW |119              |120           |103    |-7      |-6      |BDL   |CLT |644     |4     |12     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "|2006|1    |11        |3        |1642   |1645      |1841   |1845      |US           |1002     |N104UW |119              |120           |105    |-4      |-3      |BDL   |CLT |644     |4     |10     |0        |null            |0       |0           |0           |0       |0            |0                |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "sample count (limit 1000 staged):\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"smoke_test\").getOrCreate()\n",
    "\n",
    "# read just the first 1000 lines to check connectivity/schema quickly\n",
    "df = spark.read.option(\"header\",\"true\").csv(\"hdfs://namenode:8020/data/flights/2006.csv\")\n",
    "print(\"schema:\")\n",
    "df.printSchema()\n",
    "print(\"show 10 rows:\")\n",
    "df.show(10, truncate=False)\n",
    "print(\"sample count (limit 1000 staged):\")\n",
    "print(df.limit(1000).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7141922\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\",\"true\").csv(\"hdfs://namenode:8020/data/flights/2006.csv\")\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert the CSV to Parquet (typed, splittable, faster queries), then register it as a Hive external table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote parquet to: hdfs://namenode:8020/data/parquet/flights_2006\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"etl_flights\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "src = \"hdfs://namenode:8020/data/flights/2006.csv\"\n",
    "out_path = \"hdfs://namenode:8020/data/parquet/flights_2006\"\n",
    "\n",
    "# read with inferred schema\n",
    "df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src)\n",
    "\n",
    "# light cleaning example: drop rows missing Year/Month if those columns exist\n",
    "if 'Year' in df.columns and 'Month' in df.columns:\n",
    "    df_clean = df.dropna(subset=['Year','Month'])\n",
    "    df_clean.repartition(8).write.mode(\"overwrite\").partitionBy(\"Year\",\"Month\").parquet(out_path)\n",
    "else:\n",
    "    df_clean = df\n",
    "    df_clean.repartition(8).write.mode(\"overwrite\").parquet(out_path)\n",
    "\n",
    "print(\"Wrote parquet to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet rows: 7141922\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"count_parquet\").getOrCreate()\n",
    "dfp = spark.read.parquet(\"hdfs://namenode:8020/data/parquet/flights_2006\")\n",
    "print('parquet rows:', dfp.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example partitioning by Year and Month if present\n",
    "if 'Year' in df.columns and 'Month' in df.columns:\n",
    "    df_clean = df.dropna(subset=['Year','Month'])\n",
    "    df_clean.repartition(8).write.mode(\"overwrite\").partitionBy(\"Year\",\"Month\").parquet(out_path)\n",
    "else:\n",
    "    df.repartition(8).write.mode(\"overwrite\").parquet(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Partition the Parquet output based on Year/Month columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catalogImplementation: hive\n"
     ]
    }
   ],
   "source": [
    "# stop any existing session cleanly\n",
    "try:\n",
    "    spark\n",
    "    try:\n",
    "        spark.stop()\n",
    "    except Exception:\n",
    "        pass\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"register_with_hive\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:8020/user/hive/warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"catalogImplementation:\", spark.conf.get(\"spark.sql.catalogImplementation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV from hdfs://namenode:8020/data/flights/2006.csv\n",
      "CSV rows (sample): 5\n",
      "Partitioning by Year,Month\n",
      "Wrote parquet to: hdfs://namenode:8020/data/parquet/flights_2006\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"etl_flights_write_parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "src = \"hdfs://namenode:8020/data/flights/2006.csv\"\n",
    "out_path = \"hdfs://namenode:8020/data/parquet/flights_2006\"\n",
    "\n",
    "print(\"Reading CSV from\", src)\n",
    "df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src)\n",
    "print(\"CSV rows (sample):\", df.limit(5).count())\n",
    "\n",
    "# choose partitioning if Year/Month exist\n",
    "if 'Year' in df.columns and 'Month' in df.columns:\n",
    "    print(\"Partitioning by Year,Month\")\n",
    "    df_clean = df.dropna(subset=['Year','Month'])\n",
    "    df_clean.repartition(8).write.mode(\"overwrite\").partitionBy(\"Year\",\"Month\").parquet(out_path)\n",
    "else:\n",
    "    print(\"No Year/Month columns found – writing without partitioning\")\n",
    "    df.repartition(8).write.mode(\"overwrite\").parquet(out_path)\n",
    "\n",
    "print(\"Wrote parquet to:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Register Hive external table via Spark (enable Hive support):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created. Catalog implementation: hive\n",
      "Databases:\n",
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n",
      "Tables in default:\n",
      "+--------+-------------------+-----------+\n",
      "|database|          tableName|isTemporary|\n",
      "+--------+-------------------+-----------+\n",
      "| default|flights_2006_staged|      false|\n",
      "+--------+-------------------+-----------+\n",
      "\n",
      "Reading parquet directly to validate data is readable:\n",
      "+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+----+-----+\n",
      "|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|Year|Month|\n",
      "+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+----+-----+\n",
      "|        13|        7|   1748|      1800|   1947|      1952|           XE|     2793| N14162|              119|           112|     97|      -5|     -12|   PVD| CLE|     540|     3|     19|        0|            null|       0|           0|           0|       0|            0|                0|2006|    8|\n",
      "|        24|        4|   1422|      1318|   1609|      1455|           XE|     2807| N12921|              107|            97|     79|      74|      64|   IAH| PNS|     489|     3|     25|        0|            null|       0|          64|           0|      10|            0|                0|2006|    8|\n",
      "|        31|        4|   1954|      1950|   2103|      2105|           WN|     2290| N637SW|               69|            75|     59|      -2|       4|   PVD| BWI|     328|     2|      8|        0|            null|       0|           0|           0|       0|            0|                0|2006|    8|\n",
      "|        31|        4|   1503|      1425|   1555|      1522|           YV|     5079| N913FJ|              112|           117|     89|      33|      38|   CLT| STL|     575|     5|     18|        0|            null|       0|          33|           0|       0|            0|                0|2006|    8|\n",
      "|        27|        7|   1135|      1055|   1355|      1315|           WN|     1712|   N341|              140|           140|    115|      40|      40|   BWI| TPA|     842|     3|     22|        0|            null|       0|          40|           0|       0|            0|                0|2006|    8|\n",
      "+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Row count (spark read): 7141922\n",
      "Creating EXTERNAL table in metastore (will DROP if exists)...\n",
      "CREATE TABLE / verification failed. Full traceback below:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1546.sql.\n",
      ": org.apache.spark.sql.catalyst.parser.ParseException: \n",
      "Operation not allowed: CREATE EXTERNAL TABLE ... USING(line 1, pos 0)\n",
      "\n",
      "== SQL ==\n",
      "CREATE EXTERNAL TABLE flights_2006_staged USING PARQUET LOCATION 'hdfs://namenode:8020/data/parquet/flights_2006'\n",
      "^^^\n",
      "\n",
      "\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.operationNotAllowed(ParserUtils.scala:41)\n",
      "\tat org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1.apply(SparkSqlParser.scala:404)\n",
      "\tat org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1.apply(SparkSqlParser.scala:401)\n",
      "\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SparkSqlAstBuilder.visitCreateTable(SparkSqlParser.scala:401)\n",
      "\tat org.apache.spark.sql.execution.SparkSqlAstBuilder.visitCreateTable(SparkSqlParser.scala:55)\n",
      "\tat org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateTableContext.accept(SqlBaseParser.java:1390)\n",
      "\tat org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:18)\n",
      "\tat org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:72)\n",
      "\tat org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:72)\n",
      "\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:108)\n",
      "\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n",
      "\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-38-570df70eb32d>\", line 47, in <module>\n",
      "    spark.sql(f\"CREATE EXTERNAL TABLE {table_name} USING PARQUET LOCATION '{parquet_path}'\")\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 767, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 73, in deco\n",
      "    raise ParseException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.ParseException: \"\\nOperation not allowed: CREATE EXTERNAL TABLE ... USING(line 1, pos 0)\\n\\n== SQL ==\\nCREATE EXTERNAL TABLE flights_2006_staged USING PARQUET LOCATION 'hdfs://namenode:8020/data/parquet/flights_2006'\\n^^^\\n\"\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "# stop prior session if any (avoids \"stopped SparkContext\" issues)\n",
    "try:\n",
    "    spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Create SparkSession with Hive support and explicit metastore URI\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"register_flights_parquet\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:8020/user/hive/warehouse\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created. Catalog implementation:\", spark.conf.get(\"spark.sql.catalogImplementation\"))\n",
    "\n",
    "# quick metastore connectivity checks\n",
    "try:\n",
    "    print(\"Databases:\")\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "    print(\"Tables in default:\")\n",
    "    spark.sql(\"SHOW TABLES IN default\").show()\n",
    "except Exception:\n",
    "    print(\"Error while listing catalog objects:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "parquet_path = \"hdfs://namenode:8020/data/parquet/flights_2006\"\n",
    "table_name = \"flights_2006_staged\"\n",
    "\n",
    "# Sanity read direct from Parquet\n",
    "print(\"Reading parquet directly to validate data is readable:\")\n",
    "try:\n",
    "    df = spark.read.parquet(parquet_path)\n",
    "    df.show(5)\n",
    "    print(\"Row count (spark read):\", df.count())\n",
    "except Exception:\n",
    "    print(\"Failed to read parquet directly:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Try to create an EXTERNAL table that points to the parquet directory\n",
    "print(\"Creating EXTERNAL table in metastore (will DROP if exists)...\")\n",
    "try:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    spark.sql(f\"CREATE EXTERNAL TABLE {table_name} USING PARQUET LOCATION '{parquet_path}'\")\n",
    "    print(\"Table created. Verifying count from metastore-backed table:\")\n",
    "    spark.sql(f\"SELECT COUNT(*) AS cnt FROM {table_name}\").show()\n",
    "except Exception as e:\n",
    "    print(\"CREATE TABLE / verification failed. Full traceback below:\")\n",
    "    traceback.print_exc()\n",
    "    # attempt to print Java exception detail if available\n",
    "    try:\n",
    "        print(\"Java exception:\", e.java_exception.toString())\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.cancelAllJobs()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
